{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Documents\n",
    "\n",
    "The Reuters-21578 files are in [SGML format](http://kdd.ics.uci.edu/databases/reuters21578/README.txt). We need to split them into individual documents. There should be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reut2-000.sgm') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(corpus, 'html.parser')\n",
    "allArticles = soup.find_all('reuters')\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.split('[\\W\\d]+', s)\n",
    "\n",
    "articles = [] # stories with a body\n",
    "documents = []\n",
    "for a in allArticles:\n",
    "    if a.body:\n",
    "        articles.append(a)\n",
    "        # splits by numbers and non-words\n",
    "        documents.append(preprocess(a.body.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn each document into a vector of W dimensions, where W is the number of words in the corpus. Each component of the vector represents the term frequency inverse document frequency (TF-IDF) of one word in the document. TF-IDF is a weighting scheme based on a word's occurrence in a document (term frequency) and its uniqueness in the overall corpus (inverse document frequency). A higher TF-IDF usually means the word is more important in the corpus.\n",
    "\n",
    "There are [several different weighting schemes](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition) we could use; the ones I've chosen are as follows. \n",
    "\n",
    "For term frequency (tf) we will use a type of weighting scheme called [double normalization](https://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html). The first normalization divides a word's raw count by the maximum raw count of a word in that document. This prevents TF's bias towards longer documents. Then we normalize again by \"smoothing\", which prevents modest changes in tf from greatly affecting TF-IDF.\n",
    "\n",
    "For inverse document frequency (idf) we will use the standard logarithmically scaled frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "$$ tf = 0.4 + (1-0.4)\\frac{N(v_i, d)}{max_{v_j \\in s}N(v_j, d)} $$\n",
       "\n",
       "where $N(v_i, s)$ is the number of times word $v_i$ occurs in document $d$. We've used the common smoothing constant of 0.4.\n",
       "\n",
       "$$ idf = log \\frac{N}{N(v_i)} $$\n",
       "\n",
       "where $N$ is the number of documents in the corpus, and $N(v_i)$ is the number of documents that includes $v_i$.\n",
       "\n",
       "$$ TF\\text{-}IDF = tf \\times idf $$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "$$ tf = 0.4 + (1-0.4)\\frac{N(v_i, d)}{max_{v_j \\in s}N(v_j, d)} $$\n",
    "\n",
    "where $N(v_i, s)$ is the number of times word $v_i$ occurs in document $d$. We've used the common smoothing constant of 0.4.\n",
    "\n",
    "$$ idf = log \\frac{N}{N(v_i)} $$\n",
    "\n",
    "where $N$ is the number of documents in the corpus, and $N(v_i)$ is the number of documents that includes $v_i$.\n",
    "\n",
    "$$ TF\\text{-}IDF = tf \\times idf $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10252 unique words over 925 documents.\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for doc in documents:\n",
    "    words += doc\n",
    "words = [w for w in set(words) if len(w) > 0]\n",
    "numWords = len(words)\n",
    "\n",
    "print('There are {} unique words over {} documents.'.format(numWords, len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToLoc = {words[i]: i for i in range(numWords)} # a word to index lookup\n",
    "\n",
    "# stores number of documents a word appears in\n",
    "documentCounts = [0 for _ in range(numWords)]\n",
    "\n",
    "a = 0.4 # our smoothing constant\n",
    "\n",
    "# Calculates tf vector while building document counts\n",
    "def tf(doc):    \n",
    "    vector = [0 for _ in range(numWords)]\n",
    "    counted = defaultdict(bool) # keeps track of if we've counted the word towards document frequency\n",
    "    for word in doc:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        loc = wordToLoc[word]\n",
    "        vector[loc] += 1\n",
    "        if not counted[word]:\n",
    "            documentCounts[loc] += 1\n",
    "            counted[word] = True\n",
    "    maxTf = max(vector)\n",
    "    return [a + (1 - a) * c / maxTf for c in vector]\n",
    "\n",
    "tfVectors = [tf(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfCache = {}\n",
    "def getIdf(i):\n",
    "    if i not in idfCache:\n",
    "        idfCache[i] = math.log10(len(documents) / documentCounts[i])\n",
    "    return idfCache[i]\n",
    "\n",
    "def normalizeByIdf(vector):\n",
    "    output = []\n",
    "    for i in range(len(vector)):\n",
    "        output.append(vector[i] * getIdf(i))\n",
    "    return output\n",
    "            \n",
    "tfidf = [normalizeByIdf(v) for v in tfVectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "Now that we've converted each article into a vector, let's cluster them and see what we can find. Here is a [really good video](https://www.youtube.com/watch?v=_aWzGGNrcic) on how k-means clustering works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=lambda x: ' '.join(preprocess(x)))\n",
    "tfidf = tfidf_vectorizer.fit_transform([a.body.string for a in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = 50\n",
    "kmeans = KMeans(n_clusters=numClusters).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = {}\n",
    "\n",
    "for i in set(kmeans.labels_):\n",
    "    cluster_assignments[i] = [documents[x] for x in np.where(kmeans.labels_ == i)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  Versatile Corp said it agreed in principle to sell its Alberta based Versatile Noble Cultivators Co division to Vicon Inc of Ontario for undisclosed terms The division manufactures tillage and spraying equipment Reuter \n",
      "\n",
      "2. Armco Inc said its former European Steel Mill Merchandising department has become a unit of the parent s specialty steels division based in Butler The unit newly named Specialty Steels Europe is based in Cologne West Germany It will market and sell in Europe the division s U S made\n",
      "\n",
      "3. B F Goodrich Co said it will phase out the production of aircraft tires missile and marine products and molded rubber products in Akron Ohio by the end of laying off about salaried production maintenance and support services employees The company said layoffs will start within the next few weeks\n",
      "\n",
      "4. EDO Corp said McDonnell Aircraf Co has invoked its production option calling for ejection release units for the F E aircraft The company said the contract is not to exceed mln dlrs and will be handled by its government systems division in College Point N Y The company said the\n",
      "\n",
      "5. Cybertek said it is forming a General Products division which will be located in Dallas The new division will market personal computer software products targeted at the Fortune companies Cybertek said Reuter \n",
      "\n",
      "6. Ferro Corp said it has formed a joint venture with Paris based Alsthom Inudstrial Group to export U S epxertise in specialty composite materials to the European market Ferro said although the airframe and aerospace industries are the prime users of composite materials today it plans to develop applications for\n",
      "\n",
      "7. Grey Advertising Inc s GreyCom Inc subsidiary said it established a new division GreyCom Corporate and Financial Reuter \n",
      "\n",
      "8. Armtek Corp previously the Armstrong Rubber Co said it agreed to sell its industrial tire and assembly division to a Dyneer Corp DYR for an undisclosed sum It said the agreement covers the division s tire production facility in Clinton Tenn and its plants serving original equipment and replacement markets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = cluster_assignments[0]\n",
    "\n",
    "for i in range(len(cluster)):\n",
    "    print('{}. {}\\n'.format(i + 1, ' '.join(cluster[i][:50])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
