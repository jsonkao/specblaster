{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Documents\n",
    "\n",
    "We are using Reuters-21578–a small corpus of Retuers articles in [SGML format](http://kdd.ics.uci.edu/databases/reuters21578/README.txt)–to test our methodology. First, we split them into individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reut2-000.sgm') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(corpus, 'html.parser')\n",
    "allArticles = soup.find_all('reuters')\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.split('[\\W\\d]+', s)\n",
    "\n",
    "articles = [] # stories with a body\n",
    "documents = []\n",
    "for a in allArticles:\n",
    "    if a.body:\n",
    "        articles.append(a)\n",
    "        # Each document is a list of words split\n",
    "        documents.append(preprocess(a.body.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn each document into a vector of W dimensions, where W is the number of words in the corpus. Each component of the vector represents the term frequency inverse document frequency (TF-IDF) of one word in the document. TF-IDF is a weighting scheme based on a word's occurrence in a document (term frequency) and its uniqueness in the overall corpus (inverse document frequency). A higher TF-IDF usually means the word is more important in the corpus.\n",
    "\n",
    "There are [several different weighting schemes](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition) we could use; the ones I've chosen are as follows. \n",
    "\n",
    "For term frequency (tf) we will use a type of weighting scheme called [double normalization](https://nlp.stanford.edu/IR-book/html/htmledition/maximum-tf-normalization-1.html). The first normalization divides a word's raw count by the maximum raw count of a word in that document. This prevents TF's bias towards longer documents. Then we normalize again by \"smoothing\", which prevents modest changes in tf from greatly affecting TF-IDF.\n",
    "\n",
    "For inverse document frequency (idf) we will use the standard logarithmically scaled frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "$$ tf = 0.4 + (1-0.4)\\frac{N(v_i, d)}{max_{v_j \\in s}N(v_j, d)} $$\n",
       "\n",
       "where $N(v_i, s)$ is the number of times word $v_i$ occurs in document $d$. We've used the common smoothing constant of 0.4.\n",
       "\n",
       "$$ idf = log \\frac{N}{N(v_i)} $$\n",
       "\n",
       "where $N$ is the number of documents in the corpus, and $N(v_i)$ is the number of documents that includes $v_i$.\n",
       "\n",
       "$$ TF\\text{-}IDF = tf \\times idf $$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "$$ tf = 0.4 + (1-0.4)\\frac{N(v_i, d)}{max_{v_j \\in s}N(v_j, d)} $$\n",
    "\n",
    "where $N(v_i, s)$ is the number of times word $v_i$ occurs in document $d$. We've used the common smoothing constant of 0.4.\n",
    "\n",
    "$$ idf = log \\frac{N}{N(v_i)} $$\n",
    "\n",
    "where $N$ is the number of documents in the corpus, and $N(v_i)$ is the number of documents that includes $v_i$.\n",
    "\n",
    "$$ TF\\text{-}IDF = tf \\times idf $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10252 unique words over 925 documents.\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for doc in documents:\n",
    "    words += doc\n",
    "words = [w for w in set(words) if len(w) > 0]\n",
    "numWords = len(words)\n",
    "\n",
    "print(f'There are {numWords} unique words over {len(documents)} documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToLoc = {words[i]: i for i in range(numWords)} # a word to index lookup\n",
    "\n",
    "# stores number of documents a word appears in\n",
    "documentCounts = [0 for _ in range(numWords)]\n",
    "\n",
    "a = 0.4 # our smoothing constant\n",
    "\n",
    "# Calculates tf vector while building document counts\n",
    "def tf(doc):    \n",
    "    vector = [0 for _ in range(numWords)]\n",
    "    counted = defaultdict(bool) # keeps track of whether we've counted a word in document counts\n",
    "    for word in doc:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        loc = wordToLoc[word]\n",
    "        vector[loc] += 1\n",
    "        if not counted[word]:\n",
    "            documentCounts[loc] += 1\n",
    "            counted[word] = True\n",
    "    maxTf = max(vector)\n",
    "    return [a + (1 - a) * c / maxTf for c in vector]\n",
    "\n",
    "tfVectors = [tf(d) for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfCache = {}\n",
    "def getIdf(i):\n",
    "    if i not in idfCache:\n",
    "        idfCache[i] = math.log10(len(documents) / documentCounts[i])\n",
    "    return idfCache[i]\n",
    "\n",
    "def normalizeByIdf(vector):\n",
    "    return [vector[i] * getIdf(i) for i in range(len(vector))]\n",
    "            \n",
    "tfidf = [normalizeByIdf(v) for v in tfVectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "Now that we've converted each article into a vector, we can cluster them. Here is a [really good video](https://www.youtube.com/watch?v=_aWzGGNrcic) on how k-means clustering works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=lambda x: ' '.join(preprocess(x)))\n",
    "tfidf = tfidf_vectorizer.fit_transform([a.body.string for a in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = 100\n",
    "kmeans = KMeans(n_clusters=numClusters).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = {}\n",
    "\n",
    "for i in set(kmeans.labels_):\n",
    "    cluster_assignments[i] = [documents[x] for x in np.where(kmeans.labels_ == i)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. New Zealand s official foreign reserves fell to billion N Z Dlrs in January from billion dlrs in December and compared with billion a year ago period the Reserve Bank said in its weekly statistical bulletin Reuter \n",
      "\n",
      "2. South Korea plans to take steps to keep its current account surplus below five billion dlrs Economic Planning Board Minister Kim Mahn je said Kim told reporters the government would repay loans ahead of schedule and encourage firms to increase imports and investment abroad to prevent the current account surplus\n",
      "\n",
      "3. French state owned chemicals group Rhone Poulenc RHON PA said it will increase its capital with a billion franc issue of preferential investment certificates on March Company chairman Jean Rene Fourtou said mln francs of the issue will be placed in the U S Details of the issue will be\n",
      "\n",
      "4. Switzerland recorded last year its first overall surplus in government finances since ending with a net gain worth mln Swiss francs the Finance Ministry said The surplus including cash transactions and long term investments contrasted with the shortfall of billion francs and the mln franc deficit proposed in the budget\n",
      "\n",
      "5. Parent Company net profit mln Swiss francs vs mln Orders received billion francs vs billion Sales billion francs vs billion Group sales billion francs vs billion Group orders billion francs vs billion REUTE \n",
      "\n",
      "6. BBC AG Brown Boveri und Cie BBCZ Z said it will omit dividend in for the second consecutive year It said it planned to invite shareholders and non voting stockholders to subscribe to a warrant bond issue of around mln Swiss francs to be made after the June annual meeting\n",
      "\n",
      "7. Sight deposits by commercial banks at the Swiss National Bank rose by billion Swiss francs to billion in the last days of February the National Bank said Foreign exchange reserves fell billion francs to billion Sight deposits are an important measure of Swiss money market liquidity The decline in foreign\n",
      "\n",
      "8. Swiss capital exports rose to billion francs in January after billion in December and a year earlier billion the Swiss National Bank said New bond issues accounted for billion of the total after December s billion and credits mln after mln In January before the National Bank ended the distinction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = cluster_assignments[10]\n",
    "\n",
    "for i in range(len(cluster)):\n",
    "    print('{}. {}\\n'.format(i + 1, ' '.join(cluster[i][:50])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
